{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Based Models (EBM) - MNIST Generation and Classification\n",
    "\n",
    "This notebook provides a guided walkthrough for running the Energy-Based Model (EBM) project. It covers data loading, model definition, training, and visualization of both real and generated images using Langevin dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll set up the environment by importing necessary libraries and loading the project's configuration. The `config.py` file centralizes all hyperparameters, making it easy to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the project root is in the path for imports\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "from config import Config\n",
    "from src.data_loader import get_mnist_loaders\n",
    "from src.models import EnergyNet\n",
    "from src.training import train_ebm, eval_ebm, loss_function\n",
    "from src.utils import visualize_real, visualize_generated, energy_gradient, langevin_dynamics_step, sample\n",
    "\n",
    "# Setup logging to console (optional, main.py logs to file as well)\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "logger.info(f\"Current device: {config.DEVICE}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "We'll load the MNIST dataset, apply transformations (resizing and normalization to `[-1, 1]`), and create `DataLoader` instances for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Loading MNIST data...\")\n",
    "trainloader, testloader = get_mnist_loaders(\n",
    "    root=config.DATA_ROOT,\n",
    "    image_size=config.IMAGE_SIZE,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    device=config.DEVICE\n",
    ")\n",
    "logger.info(f\"MNIST data loaded. Train: {len(trainloader.dataset)} samples, Test: {len(testloader.dataset)} samples.\")\n",
    "\n",
    "# Visualize a batch of real images\n",
    "logger.info(\"Visualizing real images from the dataset...\")\n",
    "visualize_real(trainloader, num_images=16, save_path=os.path.join(config.FIGURE_SAVE_PATH, \"real_images_notebook.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Model and Optimizer\n",
    "\n",
    "Here, we instantiate our `EnergyNet` model and the Adam optimizer. The model will be moved to the appropriate device (GPU, MPS, or CPU) as determined by the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EnergyNet(\n",
    "    input_dim=config.INPUT_DIM,\n",
    "    output_dim=config.NUM_CLASSES,\n",
    "    hidden_dims=config.HIDDEN_DIMS\n",
    ").to(config.DEVICE)\n",
    "logger.info(f\"Model initialized:\\n{model}\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "logger.info(f\"Optimizer initialized with learning rate: {config.LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train the EBM\n",
    "\n",
    "We will now train the EBM using the `train_ebm` function. This function handles the training loop, loss calculation, optimization, and periodic visualization of generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Starting EBM training...\")\n",
    "train_losses, val_losses = train_ebm(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_loader=trainloader,\n",
    "    test_loader=testloader,\n",
    "    epochs=config.EPOCHS,\n",
    "    eta=config.LANGEVIN_STEPS,\n",
    "    alpha=config.LANGEVIN_ALPHA,\n",
    "    sigma=config.LANGEVIN_SIGMA,\n",
    "    device=config.DEVICE,\n",
    "    visualization_freq=config.VISUALIZATION_FREQ,\n",
    "    figure_save_path=config.FIGURE_SAVE_PATH\n",
    ")\n",
    "logger.info(\"EBM training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Final Generated Images and Loss Curves\n",
    "\n",
    "After training, we'll visualize the final set of generated images and plot the training and validation loss curves to assess the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Visualizing final generated images...\")\n",
    "visualize_generated(\n",
    "    model=model,\n",
    "    eta=config.LANGEVIN_STEPS,\n",
    "    alpha=config.LANGEVIN_ALPHA,\n",
    "    sigma=config.LANGEVIN_SIGMA,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    image_size=config.IMAGE_SIZE,\n",
    "    device=config.DEVICE,\n",
    "    save_path=os.path.join(config.FIGURE_SAVE_PATH, \"final_generated_images_notebook.png\")\n",
    ")\n",
    "\n",
    "logger.info(\"Plotting loss curves...\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, config.EPOCHS + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, config.EPOCHS + 1), val_losses, label=\"Validation Loss\", linestyle='--')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curves\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(config.FIGURE_SAVE_PATH, \"loss_curves_notebook.png\"))\n",
    "plt.show()\n",
    "logger.info(\"Loss curves plotted and saved.\")\n",
    "\n",
    "# Optionally save the trained model\n",
    "model_save_path = os.path.join(config.MODEL_SAVE_PATH, \"ebm_model_notebook.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "logger.info(f\"Trained model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiment with Hyperparameters\n",
    "\n",
    "Feel free to modify the parameters in `config.py` (or directly in the cells above if you re-run them) and re-execute the training and visualization steps to observe their impact on the EBM's performance and the quality of generated samples. Key parameters to experiment with include:\n",
    "\n",
    "-   `LANGEVIN_STEPS` (`eta`): Number of steps for Langevin dynamics. More steps can lead to better samples but also higher computational cost.\n",
    "-   `LANGEVIN_ALPHA` (`alpha`): Step size for Langevin dynamics. A larger value allows faster movement but can lead to instability.\n",
    "-   `LANGEVIN_SIGMA` (`sigma`): Noise level for Langevin dynamics. Higher noise encourages more exploration of the energy landscape.\n",
    "-   `EPOCHS` and `LEARNING_RATE` for the main training loop.\n",
    "-   `HIDDEN_DIMS` in `EnergyNet` to adjust model capacity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}